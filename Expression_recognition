#USED RESNET50
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.preprocessing import image_dataset_from_directory
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import numpy as np
import os

# image size expected by ResNet
img_size = (224, 224)

# number of images processed at once
batch_size = 32

# epochs for transfer learning phase
transfer_epochs = 5

# epochs for fine tuning phase
fine_tune_epochs = 5

# path where FER dataset is stored
# folder must contain train, val and test subfolders
fer_path = r"E:/fer"

# load training dataset
train_ds = image_dataset_from_directory(
    os.path.join(fer_path, "train"),
    image_size=img_size,
    batch_size=batch_size
)

# load validation dataset
val_ds = image_dataset_from_directory(
    os.path.join(fer_path, "val"),
    image_size=img_size,
    batch_size=batch_size
)

# load test dataset (no shuffle for evaluation)
test_ds = image_dataset_from_directory(
    os.path.join(fer_path, "test"),
    image_size=img_size,
    batch_size=batch_size,
    shuffle=False
)

# extract class names
class_names = train_ds.class_names
num_classes = len(class_names)

print("emotion classes:", class_names)

# improve performance by prefetching
autotune = tf.data.AUTOTUNE
train_ds = train_ds.prefetch(autotune)
val_ds = val_ds.prefetch(autotune)
test_ds = test_ds.prefetch(autotune)

# data augmentation to reduce overfitting
data_augmentation = models.Sequential([
    layers.RandomFlip("horizontal"),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1)
])

# load pretrained ResNet50 model
base_model = ResNet50(
    weights="imagenet",
    include_top=False,
    input_shape=(224, 224, 3)
)

# freeze base model for transfer learning
base_model.trainable = False

# build full model
model = models.Sequential([
    data_augmentation,
    layers.Rescaling(1./255),
    base_model,
    layers.GlobalAveragePooling2D(),
    layers.BatchNormalization(),
    layers.Dense(128, activation="relu"),
    layers.Dropout(0.5),
    layers.Dense(num_classes, activation="softmax")
])

# compile model for transfer learning
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

print("starting transfer learning")
model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=transfer_epochs
)

# enable fine tuning
base_model.trainable = True

# freeze lower layers and train higher layers
for layer in base_model.layers[:-40]:
    layer.trainable = False

# compile model again with lower learning rate
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

print("starting fine tuning")
model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=fine_tune_epochs
)

# lists to store predictions
y_true = []
y_pred = []

# evaluate model on test data
for images, labels in test_ds:
    predictions = model.predict(images)
    y_true.extend(labels.numpy())
    y_pred.extend(np.argmax(predictions, axis=1))

# print precision, recall, f1 score and accuracy
print("classification report")
print(classification_report(y_true, y_pred, target_names=class_names))

# print confusion matrix
cm = confusion_matrix(y_true, y_pred)
print("confusion matrix")
print(cm)

# show sample predictions
plt.figure(figsize=(9, 9))
for images, labels in test_ds.take(1):
    preds = model.predict(images)
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))
        predicted = class_names[np.argmax(preds[i])]
        actual = class_names[labels[i]]
        plt.title(f"pred: {predicted}\ntrue: {actual}")
        plt.axis("off")
plt.show()

