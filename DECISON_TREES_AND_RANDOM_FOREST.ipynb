{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMMM20Jb+2J0ylcpUCCWMIc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Atharv09007/Cryptonite_taskphas_1/blob/main/DECISON_TREES_AND_RANDOM_FOREST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVLvlePYEWbW"
      },
      "outputs": [],
      "source": [
        "I WASNT ABLE TO FIND THE FRAUD DETECTION DATA HENCE THE PROGRAMME IS WRITTEN FOR IRIS DATA SET THE LOGICS WILL REMAIN THE SAME(have used a little bit of AI for scratch implementation(not in taskphase) to check for syntax, formatting and undo errors)\n",
        "THE mathematical aspect and logic is understood and written by me\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pprint import pprint\n",
        "from sklearn.datasets import load_iris\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "pprint(df.head())import pandas as pd\n",
        "import math\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_iris\n",
        "from pprint import pprint\n",
        "import numpy as np\n",
        "\n",
        "#  Load Iris dataset\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "pprint(df.head())\n",
        "\n",
        "# Entropy function\n",
        "def calculate_entropy(labels):\n",
        "    label_counts = Counter(labels)\n",
        "    total_samples = len(labels)\n",
        "    entropy = 0\n",
        "    for count in label_counts.values():\n",
        "        p_i = count / total_samples\n",
        "        entropy -= p_i * math.log2(p_i)\n",
        "    return entropy\n",
        "\n",
        "# Function to calculate Information Gain for a feature\n",
        "def information_gain(df, feature, target='target'):\n",
        "    # Original entropy before splitting\n",
        "    total_entropy = calculate_entropy(df[target])\n",
        "\n",
        "    # Possible split points (midpoints between sorted unique values)\n",
        "    values = sorted(df[feature].unique())\n",
        "    split_points = [(values[i] + values[i+1])/2 for i in range(len(values)-1)]\n",
        "\n",
        "    best_ig = -1\n",
        "    best_split = None\n",
        "\n",
        "    # Check all split points\n",
        "    for split in split_points:\n",
        "        # Split dataset into two groups\n",
        "        left_split = df[df[feature] <= split]\n",
        "        right_split = df[df[feature] > split]\n",
        "\n",
        "        # Weighted entropy after split\n",
        "++\n",
        "        left_weight = len(left_split) / len(df)\n",
        "        right_weight = len(right_split) / len(df)\n",
        "        post_entropy = left_weight * calculate_entropy(left_split[target]) + \\\n",
        "                       right_weight * calculate_entropy(right_split[target])\n",
        "\n",
        "        # Information Gain\n",
        "        ig = total_entropy - post_entropy\n",
        "\n",
        "        # Keep the best IG and corresponding split point\n",
        "        if ig > best_ig:\n",
        "            best_ig = ig\n",
        "            best_split = split\n",
        "\n",
        "    return best_ig, best_split\n",
        "\n",
        "# Compute IG for all features\n",
        "feature_igs = {}\n",
        "for feature in iris.feature_names:\n",
        "    ig, split = information_gain(df, feature)\n",
        "    feature_igs[feature] = (ig, split)\n",
        "\n",
        "# Step 1: Find the feature with highest IG (root node)\n",
        "root_feature = max(feature_igs, key=lambda k: feature_igs[k][0])\n",
        "root_ig, root_split = feature_igs[root_feature]\n",
        "\n",
        "print(\"\\nInformation Gain for each feature:\")\n",
        "for f, (ig, split) in feature_igs.items():\n",
        "    print(f\"{f}: IG={ig:.4f}, Best Split={split:.2f}\")\n",
        "\n",
        "print(f\"\\nRoot node should be: '{root_feature}' with IG={root_ig:.4f} at split={root_split:.2f}\")\n",
        "\n",
        "\n",
        "FROM SKLEARN (done properly by me)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/mlg-ulb/creditcardfraud/master/creditcard.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "print(\"Shape of dataset:\", df.shape)\n",
        "print(df['Class'].value_counts())\n",
        "\n",
        "#Feature and Target split\n",
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "#Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "#Decision Tree Model\n",
        "dt = DecisionTreeClassifier(\n",
        "    criterion='entropy',\n",
        "    max_depth=5,\n",
        "    random_state=42\n",
        ")\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "\n",
        "#Random Forest Model\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,      # number of trees\n",
        "    criterion='entropy',   # same splitting criterion\n",
        "    max_depth=8,           # slightly deeper trees\n",
        "    random_state=42,\n",
        "    n_jobs=-1              # use all CPU cores\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "#Evaluation\n",
        "print(\"\\n Decision Tree Report \")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "\n",
        "print(\"\\n Random Forest Report \")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# Step 1: Confusion Matrices\n",
        "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_dt), annot=True, fmt='d', cmap='Blues', ax=ax[0])\n",
        "ax[0].set_title('Decision Tree Confusion Matrix')\n",
        "sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Greens', ax=ax[1])\n",
        "ax[1].set_title('Random Forest Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "importances_dt = pd.Series(dt.feature_importances_, index=X.columns)\n",
        "importances_rf = pd.Series(rf.feature_importances_, index=X.columns)\n",
        "\n",
        "# Plot top 10 for Random Forest\n",
        "top_features_rf = importances_rf.sort_values(ascending=False)[:10]\n",
        "plt.figure(figsize=(8,5))\n",
        "sns.barplot(x=top_features_rf.values, y=top_features_rf.index, palette='viridis')\n",
        "plt.title(\"Top 10 Important Features - Random Forest\")\n",
        "plt.show()\n",
        "\n",
        "# Visualize Decision Tree (optional)\n",
        "plt.figure(figsize=(18,10))\n",
        "plot_tree(dt, filled=True, feature_names=X.columns[:10], class_names=['Non-Fraud', 'Fraud'])\n",
        "plt.show()"
      ]
    }
  ]
}